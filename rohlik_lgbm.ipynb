{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:05:37.004754Z","iopub.execute_input":"2025-02-25T14:05:37.005239Z","iopub.status.idle":"2025-02-25T14:05:38.316203Z","shell.execute_reply.started":"2025-02-25T14:05:37.005202Z","shell.execute_reply":"2025-02-25T14:05:38.314998Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:47:21.330259Z","iopub.execute_input":"2025-02-25T13:47:21.330752Z","iopub.status.idle":"2025-02-25T13:47:22.590093Z","shell.execute_reply.started":"2025-02-25T13:47:21.330714Z","shell.execute_reply":"2025-02-25T13:47:22.588725Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load data into DataFrames\ncalendar = pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv\")\ntest_weights = pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv\")\ninventory = pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv\")\nsales_train = pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv\")\nsales_test = pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv\")\nsolution = pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv\")\n\n# Display the first few rows of each DataFrame\n'''for name, df in zip([\"calendar\", \"test_weights\", \"inventory\", \"sales_train\", \"sales_test\", \"solution\"], \n                     [calendar, test_weights, inventory, sales_train, sales_test, solution]):\n    print(f\"{name} DataFrame:\")\n    print(df.head(), \"\\n\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:48:33.714232Z","iopub.execute_input":"2025-02-25T13:48:33.714768Z","iopub.status.idle":"2025-02-25T13:48:39.155983Z","shell.execute_reply.started":"2025-02-25T13:48:33.714729Z","shell.execute_reply":"2025-02-25T13:48:39.154617Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'for name, df in zip([\"calendar\", \"test_weights\", \"inventory\", \"sales_train\", \"sales_test\", \"solution\"], \\n                     [calendar, test_weights, inventory, sales_train, sales_test, solution]):\\n    print(f\"{name} DataFrame:\")\\n    print(df.head(), \"\\n\")'"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"#i define helper functions\ndef get_intersection(sales_train,df_names,dfs):\n    for df,name in zip(dfs,df_names):\n        common_columns = sales_train.columns.intersection(df.columns)\n        print(name)\n        print(common_columns)\n        print(\" \")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:48:44.334652Z","iopub.execute_input":"2025-02-25T13:48:44.335047Z","iopub.status.idle":"2025-02-25T13:48:44.340103Z","shell.execute_reply.started":"2025-02-25T13:48:44.335016Z","shell.execute_reply":"2025-02-25T13:48:44.338906Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# i want to merge the calendar and inventory dataframes to the sales_train dataframe\n#i need to identify the common columns and merge on those\nget_intersection(sales_train,['calendar','inventory','sales_train'],[calendar,inventory,sales_test])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:49:07.949328Z","iopub.execute_input":"2025-02-25T13:49:07.949787Z","iopub.status.idle":"2025-02-25T13:49:07.961565Z","shell.execute_reply.started":"2025-02-25T13:49:07.949751Z","shell.execute_reply":"2025-02-25T13:49:07.960247Z"}},"outputs":[{"name":"stdout","text":"calendar\nIndex(['date', 'warehouse'], dtype='object')\n \ninventory\nIndex(['unique_id', 'warehouse'], dtype='object')\n \nsales_train\nIndex(['unique_id', 'date', 'warehouse', 'total_orders', 'sell_price_main',\n       'type_0_discount', 'type_1_discount', 'type_2_discount',\n       'type_3_discount', 'type_4_discount', 'type_5_discount',\n       'type_6_discount'],\n      dtype='object')\n \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"sales_train = pd.merge(sales_train, calendar, on=['date','warehouse'], how='left')\nsales_train = pd.merge(sales_train, inventory, on=['unique_id','warehouse'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:49:26.992209Z","iopub.execute_input":"2025-02-25T13:49:26.992703Z","iopub.status.idle":"2025-02-25T13:49:30.741648Z","shell.execute_reply.started":"2025-02-25T13:49:26.992666Z","shell.execute_reply":"2025-02-25T13:49:30.740656Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"sales_test = pd.merge(sales_test, calendar, on=['date','warehouse'], how='left')\nsales_test = pd.merge(sales_test, inventory, on=['unique_id','warehouse'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:49:37.605239Z","iopub.execute_input":"2025-02-25T13:49:37.605709Z","iopub.status.idle":"2025-02-25T13:49:37.650296Z","shell.execute_reply.started":"2025-02-25T13:49:37.605672Z","shell.execute_reply":"2025-02-25T13:49:37.649198Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"#In these problems it is best to use the date as index\nsales_train['date']=pd.to_datetime(sales_train['date'])\nsales_train.set_index('date',inplace=True)\nsales_test['date']=pd.to_datetime(sales_test['date'])\nsales_test.set_index('date',inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:49:49.063266Z","iopub.execute_input":"2025-02-25T13:49:49.063731Z","iopub.status.idle":"2025-02-25T13:49:49.800435Z","shell.execute_reply.started":"2025-02-25T13:49:49.063691Z","shell.execute_reply":"2025-02-25T13:49:49.799286Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#i need to drop the featurees that are in the train dataset but not in the test one \nunique_columns = sales_train.columns.symmetric_difference(sales_test.columns)\nprint(unique_columns)\nsales_train.drop('availability', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:54:10.310062Z","iopub.execute_input":"2025-02-25T13:54:10.310492Z","iopub.status.idle":"2025-02-25T13:54:10.804465Z","shell.execute_reply.started":"2025-02-25T13:54:10.310459Z","shell.execute_reply":"2025-02-25T13:54:10.803551Z"}},"outputs":[{"name":"stdout","text":"Index(['availability', 'sales'], dtype='object')\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"#i skip this part","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# data preprocessing ","metadata":{}},{"cell_type":"code","source":"#for data preprocessing i convert all the object features to categorical variables\n#i identif and separate the categorical and non categorical features\n# Separate numeric and non-numeric columns\nnumeric_cols = sales_train.select_dtypes(include=['number']).columns.tolist()\nnon_numeric_cols = sales_train.select_dtypes(exclude=['number']).columns.tolist()\nprint(numeric_cols)\nprint(non_numeric_cols)\nprint(sales_train[non_numeric_cols].dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T13:59:25.163593Z","iopub.execute_input":"2025-02-25T13:59:25.164237Z","iopub.status.idle":"2025-02-25T13:59:27.974891Z","shell.execute_reply.started":"2025-02-25T13:59:25.164200Z","shell.execute_reply":"2025-02-25T13:59:27.973933Z"}},"outputs":[{"name":"stdout","text":"['unique_id', 'total_orders', 'sales', 'sell_price_main', 'type_0_discount', 'type_1_discount', 'type_2_discount', 'type_3_discount', 'type_4_discount', 'type_5_discount', 'type_6_discount', 'holiday', 'shops_closed', 'winter_school_holidays', 'school_holidays', 'product_unique_id']\n['warehouse', 'holiday_name', 'name', 'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']\nwarehouse              object\nholiday_name           object\nname                   object\nL1_category_name_en    object\nL2_category_name_en    object\nL3_category_name_en    object\nL4_category_name_en    object\ndtype: object\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"sales_train[non_numeric_cols] = sales_train[non_numeric_cols].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:00:14.819961Z","iopub.execute_input":"2025-02-25T14:00:14.820448Z","iopub.status.idle":"2025-02-25T14:00:17.215366Z","shell.execute_reply.started":"2025-02-25T14:00:14.820393Z","shell.execute_reply":"2025-02-25T14:00:17.214089Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# i want to add some new features (eg the year and the sinusoidal year)\n\nclass DateFeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        \n        # Ensure the index is a DatetimeIndex\n        if not isinstance(X.index, pd.DatetimeIndex):\n            raise ValueError(\"Index must be a DatetimeIndex\")\n        \n        # Extract basic time features\n        X[\"year\"] = X.index.year\n        X[\"month\"] = X.index.month\n        X[\"week\"] = X.index.isocalendar().week\n        \n        # Cyclic encoding for month\n        X[\"month_sin\"] = np.sin(2 * np.pi * X[\"month\"] / 12)\n        X[\"month_cos\"] = np.cos(2 * np.pi * X[\"month\"] / 12)\n        \n        # Cyclic encoding for week\n        X[\"week_sin\"] = np.sin(2 * np.pi * X[\"week\"] / 52)\n        X[\"week_cos\"] = np.cos(2 * np.pi * X[\"week\"] / 52)\n\n        return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:06:07.354339Z","iopub.execute_input":"2025-02-25T14:06:07.355134Z","iopub.status.idle":"2025-02-25T14:06:07.362843Z","shell.execute_reply.started":"2025-02-25T14:06:07.355092Z","shell.execute_reply":"2025-02-25T14:06:07.361613Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Example usage:\ntransformer = DateFeatureTransformer()\nsales_train = transformer.fit_transform(sales_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:06:19.970693Z","iopub.execute_input":"2025-02-25T14:06:19.971089Z","iopub.status.idle":"2025-02-25T14:06:21.166273Z","shell.execute_reply.started":"2025-02-25T14:06:19.971055Z","shell.execute_reply":"2025-02-25T14:06:21.165301Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Example usage:\ntransformer = DateFeatureTransformer()\nsales_test = transformer.fit_transform(sales_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:10:16.798260Z","iopub.execute_input":"2025-02-25T14:10:16.798805Z","iopub.status.idle":"2025-02-25T14:10:16.829750Z","shell.execute_reply.started":"2025-02-25T14:10:16.798761Z","shell.execute_reply":"2025-02-25T14:10:16.828515Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"sales_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:06:36.237449Z","iopub.execute_input":"2025-02-25T14:06:36.237872Z","iopub.status.idle":"2025-02-25T14:06:36.269894Z","shell.execute_reply.started":"2025-02-25T14:06:36.237837Z","shell.execute_reply":"2025-02-25T14:06:36.268593Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"            unique_id   warehouse  total_orders  sales  sell_price_main  \\\ndate                                                                      \n2024-03-10       4845  Budapest_1        6436.0  16.34           646.26   \n2021-05-25       4845  Budapest_1        4663.0  12.63           455.96   \n2021-12-20       4845  Budapest_1        6507.0  34.55           455.96   \n2023-04-29       4845  Budapest_1        5463.0  34.52           646.26   \n2022-04-01       4845  Budapest_1        5997.0  35.92           486.41   \n\n            type_0_discount  type_1_discount  type_2_discount  \\\ndate                                                            \n2024-03-10          0.00000              0.0              0.0   \n2021-05-25          0.00000              0.0              0.0   \n2021-12-20          0.00000              0.0              0.0   \n2023-04-29          0.20024              0.0              0.0   \n2022-04-01          0.00000              0.0              0.0   \n\n            type_3_discount  type_4_discount  ...  L2_category_name_en  \\\ndate                                          ...                        \n2024-03-10              0.0          0.15312  ...         Bakery_L2_18   \n2021-05-25              0.0          0.15025  ...         Bakery_L2_18   \n2021-12-20              0.0          0.15025  ...         Bakery_L2_18   \n2023-04-29              0.0          0.15312  ...         Bakery_L2_18   \n2022-04-01              0.0          0.15649  ...         Bakery_L2_18   \n\n            L3_category_name_en L4_category_name_en  year  month  week  \\\ndate                                                                     \n2024-03-10         Bakery_L3_83         Bakery_L4_1  2024      3    10   \n2021-05-25         Bakery_L3_83         Bakery_L4_1  2021      5    21   \n2021-12-20         Bakery_L3_83         Bakery_L4_1  2021     12    51   \n2023-04-29         Bakery_L3_83         Bakery_L4_1  2023      4    17   \n2022-04-01         Bakery_L3_83         Bakery_L4_1  2022      4    13   \n\n               month_sin     month_cos  week_sin  week_cos  \ndate                                                        \n2024-03-10  1.000000e+00  6.123234e-17  0.935016  0.354605  \n2021-05-25  5.000000e-01 -8.660254e-01  0.568065 -0.822984  \n2021-12-20 -2.449294e-16  1.000000e+00 -0.120537  0.992709  \n2023-04-29  8.660254e-01 -5.000000e-01  0.885456 -0.464723  \n2022-04-01  8.660254e-01 -5.000000e-01       1.0      -0.0  \n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>warehouse</th>\n      <th>total_orders</th>\n      <th>sales</th>\n      <th>sell_price_main</th>\n      <th>type_0_discount</th>\n      <th>type_1_discount</th>\n      <th>type_2_discount</th>\n      <th>type_3_discount</th>\n      <th>type_4_discount</th>\n      <th>...</th>\n      <th>L2_category_name_en</th>\n      <th>L3_category_name_en</th>\n      <th>L4_category_name_en</th>\n      <th>year</th>\n      <th>month</th>\n      <th>week</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n      <th>week_sin</th>\n      <th>week_cos</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2024-03-10</th>\n      <td>4845</td>\n      <td>Budapest_1</td>\n      <td>6436.0</td>\n      <td>16.34</td>\n      <td>646.26</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15312</td>\n      <td>...</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>2024</td>\n      <td>3</td>\n      <td>10</td>\n      <td>1.000000e+00</td>\n      <td>6.123234e-17</td>\n      <td>0.935016</td>\n      <td>0.354605</td>\n    </tr>\n    <tr>\n      <th>2021-05-25</th>\n      <td>4845</td>\n      <td>Budapest_1</td>\n      <td>4663.0</td>\n      <td>12.63</td>\n      <td>455.96</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15025</td>\n      <td>...</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>2021</td>\n      <td>5</td>\n      <td>21</td>\n      <td>5.000000e-01</td>\n      <td>-8.660254e-01</td>\n      <td>0.568065</td>\n      <td>-0.822984</td>\n    </tr>\n    <tr>\n      <th>2021-12-20</th>\n      <td>4845</td>\n      <td>Budapest_1</td>\n      <td>6507.0</td>\n      <td>34.55</td>\n      <td>455.96</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15025</td>\n      <td>...</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>2021</td>\n      <td>12</td>\n      <td>51</td>\n      <td>-2.449294e-16</td>\n      <td>1.000000e+00</td>\n      <td>-0.120537</td>\n      <td>0.992709</td>\n    </tr>\n    <tr>\n      <th>2023-04-29</th>\n      <td>4845</td>\n      <td>Budapest_1</td>\n      <td>5463.0</td>\n      <td>34.52</td>\n      <td>646.26</td>\n      <td>0.20024</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15312</td>\n      <td>...</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>2023</td>\n      <td>4</td>\n      <td>17</td>\n      <td>8.660254e-01</td>\n      <td>-5.000000e-01</td>\n      <td>0.885456</td>\n      <td>-0.464723</td>\n    </tr>\n    <tr>\n      <th>2022-04-01</th>\n      <td>4845</td>\n      <td>Budapest_1</td>\n      <td>5997.0</td>\n      <td>35.92</td>\n      <td>486.41</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15649</td>\n      <td>...</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>2022</td>\n      <td>4</td>\n      <td>13</td>\n      <td>8.660254e-01</td>\n      <td>-5.000000e-01</td>\n      <td>1.0</td>\n      <td>-0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\n\n# Identify numeric and categorical columns\n# Exclude 'sales' from numeric columns\nnumeric_cols = [col for col in sales_train.select_dtypes(include=['number']).columns if col != \"sales\"]\nnon_numeric_cols = sales_train.select_dtypes(exclude=['number']).columns.tolist()\nprint(numeric_cols)\nprint(non_numeric_cols)\n\n# Define preprocessing steps\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),  # Fill NA with -1\n    (\"scaler\", StandardScaler())  # Normalize numeric features\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))  # Encode categorical features\n])\n\n# Combine into a ColumnTransformer\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", numeric_transformer, numeric_cols),\n    (\"cat\", categorical_transformer, non_numeric_cols)\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:22:47.581004Z","iopub.execute_input":"2025-02-25T14:22:47.581570Z","iopub.status.idle":"2025-02-25T14:22:48.271454Z","shell.execute_reply.started":"2025-02-25T14:22:47.581523Z","shell.execute_reply":"2025-02-25T14:22:48.270265Z"}},"outputs":[{"name":"stdout","text":"['unique_id', 'total_orders', 'sell_price_main', 'type_0_discount', 'type_1_discount', 'type_2_discount', 'type_3_discount', 'type_4_discount', 'type_5_discount', 'type_6_discount', 'holiday', 'shops_closed', 'winter_school_holidays', 'school_holidays', 'product_unique_id', 'year', 'month', 'week', 'month_sin', 'month_cos', 'week_sin', 'week_cos']\n['warehouse', 'holiday_name', 'name', 'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# train an LGBM model on ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit, cross_val_score\nfrom lightgbm import LGBMRegressor\n\n# Define target and features\ny = sales_train[\"sales\"]\nX = sales_train.drop(columns=[\"sales\"])\n\n# Define TimeSeriesSplit (test window = 14 days)\ntscv = TimeSeriesSplit(n_splits=5, test_size=14)  # Adjust n_splits based on your data size\n\n# Initialize LGBM model\nmodel = LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n\n# Impute missing values in y\ny_imputer = SimpleImputer(strategy=\"mean\")  # Options: \"mean\", \"median\", \"constant\", etc.\ny = pd.Series(y_imputer.fit_transform(y.values.reshape(-1, 1)).flatten(), index=y.index)\n\n# Apply cross-validation\ncv_scores = cross_val_score(model, preprocessor.fit_transform(X), y, cv=tscv, scoring=\"neg_mean_absolute_error\")\n\n# Print results\nprint(\"Cross-validation MAE scores:\", -cv_scores)\nprint(\"Mean MAE:\", -np.mean(cv_scores))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:32:44.988950Z","iopub.execute_input":"2025-02-25T14:32:44.989495Z","iopub.status.idle":"2025-02-25T14:34:41.469931Z","shell.execute_reply.started":"2025-02-25T14:32:44.989451Z","shell.execute_reply":"2025-02-25T14:34:41.468577Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.342496 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2889\n[LightGBM] [Info] Number of data points in the train set: 4007349, number of used features: 29\n[LightGBM] [Info] Start training from score 108.381566\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.335463 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2890\n[LightGBM] [Info] Number of data points in the train set: 4007363, number of used features: 29\n[LightGBM] [Info] Start training from score 108.381408\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.319911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2894\n[LightGBM] [Info] Number of data points in the train set: 4007377, number of used features: 29\n[LightGBM] [Info] Start training from score 108.381343\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.404943 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2896\n[LightGBM] [Info] Number of data points in the train set: 4007391, number of used features: 29\n[LightGBM] [Info] Start training from score 108.381319\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.340773 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2883\n[LightGBM] [Info] Number of data points in the train set: 4007405, number of used features: 29\n[LightGBM] [Info] Start training from score 108.381027\nCross-validation MAE scores: [31.58989871 39.71970945 69.61325691 29.50168852 95.99637618]\nMean MAE: 53.284185956007114\n","output_type":"stream"}],"execution_count":32}]}